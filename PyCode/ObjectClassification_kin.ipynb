{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a94b42-b23b-4c70-98ad-ae87e1a98245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import glmnet_python\n",
    "# from glmnet_python import glmnet\n",
    "# from glmnet_python import glmnetPredict\n",
    "# from glmnet_python import glmnetPlot\n",
    "# from glmnet_python import cvglmnet\n",
    "# from glmnet_python import cvglmnetPredict\n",
    "# from glmnet_python import cvglmnetPlot\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af78493b-84af-46cd-9742-f9ca13de89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES TO ADJUST\n",
    "# 'Mugs', 'Plates', 'Geometric', 'Cutlery', 'Ball'\n",
    "family_to_select = 'Ball'\n",
    "num_bins = 20\n",
    "cv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2a31b9-94d3-42e3-bd63-be6469db73d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READING FILES AND CREATING DATASET\n",
    "file_data = \"./PyData/filtered_data.json\"\n",
    "file_eps = \"./PyData/ep_labels.json\"\n",
    "file_task = \"./PyData/task_labels.json\"\n",
    "file_allSyn = \"./PyData/all_subjects_scores.json\"\n",
    "file_emg = \"./PyData/emg_data.json\"\n",
    "\n",
    "with open(file_data, \"r\") as f:\n",
    "    data = json.load(f)  # data[subjects][trials][joints]\n",
    "\n",
    "with open(file_eps, \"r\") as g:\n",
    "    eps = json.load(g)  # eps[subjects][trials]\n",
    "\n",
    "with open(file_task, \"r\") as h:\n",
    "    task = json.load(h)  # task[subjects][trials]\n",
    "\n",
    "with open(file_allSyn, \"r\") as h:\n",
    "    all_syn = json.load(h)  # all_syn[subjects X trials][joints]\n",
    "    \n",
    "with open(file_emg, \"r\") as f:\n",
    "    emg_data = json.load(f)  # data[subjects][trials][sensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17952a0b-0777-4983-917c-75f70cb0c92e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VECTORIZING DATA\n",
    "\n",
    "vectorized_task = [x for sublist in task for x in sublist]  # Vectorization of tasks\n",
    "vectorized_eps = [x for sublist in eps for x in sublist]  # Vectorization of eps\n",
    "vectorized_data = [x for sublist in data for x in sublist]  # Vectorization of trials\n",
    "vectorized_data = np.array(vectorized_data, dtype=float) # Conversion to float to we replace 'None' with 'NaN'\n",
    "vectorized_emg_data = [x for sublist in emg_data for x in sublist]  # Vectorization of trials\n",
    "vectorized_emg_data = np.array(vectorized_emg_data, dtype=float) # Conversion to float to we replace 'None' with 'NaN'\n",
    "given_object = [x.split(\"_\")[0] for x in vectorized_task]  # Vectorized given objects\n",
    "ask_object = [x.split(\"_\")[1] for x in vectorized_task]  # Vectorized asked objects\n",
    "all_syn = np.array(all_syn, dtype=float) # Conversion to float to replace 'None' with 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d818fa-4ff8-4f40-9256-9d32ce86e41b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SPLIT BY TRIALS\n",
    "\n",
    "tr_idx = [index for index, _ in enumerate(vectorized_task) if vectorized_task[index] != vectorized_task[index-1]]\n",
    "tr_idx.append(len(vectorized_task))\n",
    "\n",
    "# all these lists are [trials]x[timepoints per trial]\n",
    "spl_task = [vectorized_task[tr_idx[i]:tr_idx[i+1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx)-1]\n",
    "spl_eps = [vectorized_eps[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_dat = [vectorized_data[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_emg = [vectorized_emg_data[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_given = [given_object[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_ask = [ask_object[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_syn = [all_syn[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8930f5cc-ac73-4285-881f-acd31406dfd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SELECT TRIALS BY FAMILY\n",
    "\n",
    "obj_fam = dict(\n",
    "    CeramicMug = 'Mugs',\n",
    "    Glass = 'Mugs',\n",
    "    MetalMug = 'Mugs',\n",
    "    CeramicPlate = 'Plates',\n",
    "    MetalPlate = 'Plates',\n",
    "    PlasticPlate = 'Plates',\n",
    "    Cube = 'Geometric',\n",
    "    Cylinder ='Geometric',\n",
    "    Triangle ='Geometric',\n",
    "    Fork = 'Cutlery',\n",
    "    Knife ='Cutlery',\n",
    "    Spoon ='Cutlery',\n",
    "    PingPongBall = 'Ball',\n",
    "    SquashBall='Ball',\n",
    "    TennisBall='Ball'\n",
    ")\n",
    "\n",
    "fam_idx = list()\n",
    "for it in range(len(spl_given)):\n",
    "    if obj_fam[spl_given[it][0]] == family_to_select:\n",
    "        fam_idx.append(it)\n",
    "\n",
    "selected_task = [spl_task[idx] for idx in fam_idx]\n",
    "selected_eps = [spl_eps[idx] for idx in fam_idx]\n",
    "selected_dat = [spl_dat[idx] for idx in fam_idx]\n",
    "selected_emg = [spl_emg[idx] for idx in fam_idx]\n",
    "selected_given = [spl_given[idx] for idx in fam_idx]\n",
    "selected_ask = [spl_ask[idx] for idx in fam_idx]\n",
    "selected_syn = [spl_syn[idx] for idx in fam_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80cab919-dfee-429c-84a9-f58e8a07f3f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9297/3717972689.py:42: RuntimeWarning: Mean of empty slice\n",
      "  dat_mean = [np.nanmean(x, axis=0) for x in div_dat]\n",
      "/tmp/ipykernel_9297/3717972689.py:43: RuntimeWarning: Mean of empty slice\n",
      "  syn_mean = [np.nanmean(x, axis=0) for x in div_syn]\n",
      "/tmp/ipykernel_9297/3717972689.py:44: RuntimeWarning: Mean of empty slice\n",
      "  emg_mean = [np.nanmean(x, axis=0) for x in div_emg]\n"
     ]
    }
   ],
   "source": [
    "# DIVIDE BY EPs\n",
    "\n",
    "# all these list are [trials]x[eps per trial]x[timepoints per ep]\n",
    "final_task = list()\n",
    "final_eps = list()\n",
    "final_given = list()\n",
    "final_ask = list()\n",
    "# this lists are [trials]x[eps per trial]x[bins per ep]x[joints]\n",
    "final_dat = list()\n",
    "final_emg = list()\n",
    "final_syn = list()\n",
    "\n",
    "for e in range(len(selected_eps)):\n",
    "\n",
    "    ch_idx = [index for index, _ in enumerate(selected_eps[e]) if selected_eps[e][index] != selected_eps[e][index-1]]\n",
    "    ch_idx.append(len(selected_eps[e]))\n",
    "    if 0 not in ch_idx:\n",
    "        ch_idx.insert(0, 0)\n",
    "\n",
    "    sel_task = [selected_task[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_task.append(sel_task)\n",
    "    sel_eps = [selected_eps[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_eps.append(sel_eps)\n",
    "    sel_given = [selected_given[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_given.append(sel_given)\n",
    "    sel_ask = [selected_ask[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_ask.append(sel_ask)\n",
    "\n",
    "    sel_dat = [selected_dat[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    sel_emg = [selected_emg[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    sel_syn = [selected_syn[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    \n",
    "    # DIVIDE BY BINS\n",
    "    aux = list()\n",
    "    aux_syn = list()\n",
    "    aux_emg = list()\n",
    "    for j in range(len(sel_dat)):\n",
    "        div_dat = np.array_split(sel_dat[j], num_bins)\n",
    "        div_emg = np.array_split(sel_emg[j], num_bins)\n",
    "        div_syn = np.array_split(sel_syn[j], num_bins)\n",
    "        \n",
    "        dat_mean = [np.nanmean(x, axis=0) for x in div_dat]\n",
    "        syn_mean = [np.nanmean(x, axis=0) for x in div_syn]\n",
    "        emg_mean = [np.nanmean(x, axis=0) for x in div_emg]\n",
    "        \n",
    "        aux.append(dat_mean)\n",
    "        aux_syn.append(syn_mean)\n",
    "        aux_emg.append(emg_mean)\n",
    "      \n",
    "    final_dat.append(aux)\n",
    "    final_syn.append(aux_syn)\n",
    "    final_emg.append(aux_emg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d0ec5e-fb6c-4145-88d7-097e0e37710c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE UPDATED\n",
      "FILE UPDATED\n",
      "FILE UPDATED\n",
      "|| FAMILY:  Ball  || Bins per EP:  20  ||\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# SELECT TRIALS CV\n",
    "\n",
    "total_acc = list()\n",
    "unique_given = [final_given[i][0][0] for i in range(len(final_given))]  # unique object per trial\n",
    "skf = StratifiedKFold(n_splits=cv)\n",
    "for train, test in skf.split(final_dat, unique_given):\n",
    "\n",
    "    train_given = [unique_given[x] for x in train]\n",
    "    test_given = [unique_given[y] for y in test]\n",
    "    train_data = [final_dat[x] for x in train]\n",
    "    test_data = [final_dat[y] for y in test]\n",
    "    train_emg = [final_emg[x] for x in train]\n",
    "    test_emg = [final_emg[y] for y in test]\n",
    "    train_syn = [final_syn[x] for x in train]\n",
    "    test_syn = [final_syn[y] for y in test]\n",
    "\n",
    "    # FOR SINGLE SOURCE CLASSIFICATION\n",
    "    trn_dat = list()\n",
    "    trn_emg = list()\n",
    "    trn_lab = list()\n",
    "    trn_syn = list()\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        for j in range(len(train_data[i])):\n",
    "            aux = list()\n",
    "            aux_syn = list()\n",
    "            aux_emg = list()\n",
    "            for k in range(len(train_data[i][j])):\n",
    "                aux = np.append(aux, train_data[i][j][k])\n",
    "                aux_syn = np.append(aux_syn, train_syn[i][j][k])\n",
    "                aux_emg = np.append(aux_emg, train_emg[i][j][k])\n",
    "            if np.count_nonzero(np.isnan(aux)) == 0 and np.count_nonzero(np.isnan(aux_syn)) == 0 and np.count_nonzero(np.isnan(aux_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "                trn_dat.append(aux)  # size = Joints X bins\n",
    "                trn_lab.append(train_given[i])\n",
    "                trn_syn.append(aux_syn)  # size = Synergies X bins\n",
    "                trn_emg.append(aux_emg)  # size = Electrodes X bins\n",
    "\n",
    "    tst_dat = list()\n",
    "    tst_emg = list()\n",
    "    tst_lab = list()\n",
    "    tst_syn = list()\n",
    "    for i2 in range(len(test_data)):\n",
    "        for j2 in range(len(test_data[i2])):\n",
    "            aux2 = list()\n",
    "            aux2_syn = list()\n",
    "            aux2_emg = list()\n",
    "            for k2 in range(len(test_data[i2][j2])):\n",
    "                aux2 = np.append(aux2, test_data[i2][j2][k2])\n",
    "                aux2_syn = np.append(aux2_syn, test_syn[i2][j2][k2])\n",
    "                aux2_emg = np.append(aux2_emg, test_emg[i2][j2][k2])\n",
    "            if np.count_nonzero(np.isnan(aux2)) == 0 and np.count_nonzero(np.isnan(aux2_syn)) == 0 and np.count_nonzero(np.isnan(aux2_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "                tst_dat.append(aux2)\n",
    "                tst_lab.append(test_given[i2])\n",
    "                tst_syn.append(aux2_syn)\n",
    "                tst_emg.append(aux2_emg)\n",
    "                \n",
    "    # USING LOGISTIC REGRESSION\n",
    "    log_model = LogisticRegression(penalty='elasticnet', C=0.2, solver='saga', max_iter=25000, multi_class='multinomial', l1_ratio=0.5)\n",
    "    log_model.fit(trn_dat, trn_lab)\n",
    "#     pred = log_model.predict_proba(tst_dat)\n",
    "\n",
    "#     cl = log_model.classes_\n",
    "#     hits = 0\n",
    "#     for i in range(len(pred)):\n",
    "#         if cl[np.argmax(pred[i])] == tst_lab[i]:\n",
    "#             hits += 1\n",
    "#     acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     total_acc.append(acc)\n",
    "#     print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "#     #  USING GLMNET\n",
    "#     # lbl = LabelEncoder()\n",
    "#     # dum = np.float64(lbl.fit_transform(trn_lab))\n",
    "#     #\n",
    "#     # cvfit = cvglmnet(x=np.array(trn_dat), y=dum, family='multinomial', alpha=0.5)\n",
    "#     # pred = cvglmnetPredict(cvfit, np.array(tst_dat), ptype='class')\n",
    "#     #\n",
    "#     # dum_res = np.float64(lbl.fit_transform(tst_lab))\n",
    "#     # hits = 0\n",
    "#     # for i in range(len(pred)):\n",
    "#     #     if pred[i] == dum_res[i]:\n",
    "#     #         hits += 1\n",
    "#     # acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     # total_acc.append(acc)\n",
    "#     # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "#     # USING SVM\n",
    "#     # clf = svm.SVC(C=25, kernel='poly', degree=3, decision_function_shape='ovo')  # 45.91\n",
    "#     # clf.fit(np.array(trn_dat), trn_lab)\n",
    "#     # pred = clf.predict(np.array(tst_dat))\n",
    "#     # hits = 0\n",
    "#     # for i in range(len(pred)):\n",
    "#     #     if pred[i] == tst_lab[i]:\n",
    "#     #         hits += 1\n",
    "#     # acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     # total_acc.append(acc)\n",
    "#     # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "#     # USING ANN\n",
    "#     # num_hidden_layers = 4\n",
    "#     # layers = np.ceil(np.geomspace(len(trn_dat[0]), 3, num_hidden_layers+2)).astype(int)\n",
    "#     # clf = MLPClassifier(solver='adam', alpha=20, hidden_layer_sizes=(layers[1:len(layers)-1]), random_state=1, max_iter=100000, activation='tanh')\n",
    "#     # clf.fit(np.array(trn_dat), trn_lab)\n",
    "#     # pred = clf.predict(np.array(tst_dat))\n",
    "#     # hits = 0\n",
    "#     # for i in range(len(pred)):\n",
    "#     #     if pred[i] == tst_lab[i]:\n",
    "#     #         hits += 1\n",
    "#     # acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     # total_acc.append(acc)\n",
    "#     # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "#     # FOR MULTIPLE SOURCE CLASSIFICATION\n",
    "#     trn_dat = list()\n",
    "#     trn_lab = list()\n",
    "\n",
    "#     for i in range(len(train_data)):\n",
    "#         for j in range(len(train_data[i])):\n",
    "#             aux = list()\n",
    "#             aux_emg = list()\n",
    "#             for k in range(len(train_data[i][j])):\n",
    "#                 aux = np.append(aux, train_data[i][j][k])\n",
    "#                 aux_emg = np.append(aux_emg, train_emg[i][j][k])\n",
    "#             if np.count_nonzero(np.isnan(aux)) == 0 and np.count_nonzero(np.isnan(aux_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "#                 trn_lab.append(train_given[i])  \n",
    "#                 res = np.concatenate((aux, aux_emg), axis=None)\n",
    "#                 trn_dat.append(res)\n",
    "\n",
    "#     tst_dat = list()\n",
    "#     tst_lab = list()\n",
    "#     for i2 in range(len(test_data)):\n",
    "#         for j2 in range(len(test_data[i2])):\n",
    "#             aux2 = list()\n",
    "#             aux2_emg = list()\n",
    "#             for k2 in range(len(test_data[i2][j2])):\n",
    "#                 aux2 = np.append(aux2, test_data[i2][j2][k2])\n",
    "#                 aux2_emg = np.append(aux2_emg, test_emg[i2][j2][k2])\n",
    "#             if np.count_nonzero(np.isnan(aux2)) == 0 and np.count_nonzero(np.isnan(aux2_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "#                 res = np.concatenate((aux2, aux2_emg), axis=None)\n",
    "#                 tst_dat.append(res)\n",
    "#                 tst_lab.append(test_given[i2])\n",
    "                \n",
    "#     # USING LOGISTIC REGRESSION\n",
    "#     log_model = LogisticRegression(penalty='elasticnet', C=0.2, solver='saga', max_iter=25000, multi_class='multinomial', n_jobs=-1, l1_ratio=0.5)\n",
    "#     log_model.fit(trn_dat, trn_lab)\n",
    "#     weights = log_model.coef_\n",
    "#     # print(\"Size Weights: \", weights.shape)\n",
    "#     # print(\"Classes: \", log_model.classes_)\n",
    "    \n",
    "    # WRITE WEIGHTS INTO A FILE\n",
    "    file_name = \"./PyData/\" + family_to_select + \"_Kin.csv\"\n",
    "    weights = log_model.coef_\n",
    "    resh = weights.reshape((weights.shape[0], int(weights.shape[1]/num_bins), num_bins))\n",
    "    mean_resh = np.mean(resh, axis=2)\n",
    "    df = pd.DataFrame(mean_resh, index=log_model.classes_)\n",
    "    df.to_csv(file_name, index=True, header=None, mode='a')\n",
    "    print(\"FILE UPDATED\")\n",
    "# \n",
    "#     pred = log_model.predict_proba(tst_dat)\n",
    "# \n",
    "#     cl = log_model.classes_\n",
    "#     hits = 0\n",
    "#     for i in range(len(pred)):\n",
    "#         if cl[np.argmax(pred[i])] == tst_lab[i]:\n",
    "#             hits += 1\n",
    "#     acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     total_acc.append(acc)\n",
    "#     print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "    #  USING GLMNET\n",
    "    # lbl = LabelEncoder()\n",
    "    # dum = np.float64(lbl.fit_transform(trn_lab))\n",
    "    #\n",
    "    # cvfit = cvglmnet(x=np.array(trn_dat), y=dum, family='multinomial', alpha=0.5)\n",
    "    # pred = cvglmnetPredict(cvfit, np.array(tst_dat), ptype='class')\n",
    "    #\n",
    "    # dum_res = np.float64(lbl.fit_transform(tst_lab))\n",
    "    # hits = 0\n",
    "    # for i in range(len(pred)):\n",
    "    #     if pred[i] == dum_res[i]:\n",
    "    #         hits += 1\n",
    "    # acc = round((hits/len(tst_lab))*100, 2)\n",
    "    # total_acc.append(acc)\n",
    "    # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "    # USING SVM\n",
    "    # clf = svm.SVC(C=25, kernel='poly', degree=3, decision_function_shape='ovo')  # 45.91\n",
    "    # clf.fit(np.array(trn_dat), trn_lab)\n",
    "    # pred = clf.predict(np.array(tst_dat))\n",
    "    # hits = 0\n",
    "    # for i in range(len(pred)):\n",
    "    #     if pred[i] == tst_lab[i]:\n",
    "    #         hits += 1\n",
    "    # acc = round((hits/len(tst_lab))*100, 2)\n",
    "    # total_acc.append(acc)\n",
    "    # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "    # USING ANN\n",
    "    # num_hidden_layers = 4\n",
    "    # layers = np.ceil(np.geomspace(len(trn_dat[0]), 3, num_hidden_layers+2)).astype(int)\n",
    "    # clf = MLPClassifier(solver='adam', alpha=20, hidden_layer_sizes=(layers[1:len(layers)-1]), random_state=1, max_iter=100000, activation='tanh')\n",
    "    # clf.fit(np.array(trn_dat), trn_lab)\n",
    "    # pred = clf.predict(np.array(tst_dat))\n",
    "    # hits = 0\n",
    "    # for i in range(len(pred)):\n",
    "    #     if pred[i] == tst_lab[i]:\n",
    "    #         hits += 1\n",
    "    # acc = round((hits/len(tst_lab))*100, 2)\n",
    "    # total_acc.append(acc)\n",
    "    # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "# print(\"|| FAMILY: \", family_to_select, \" || Mean accuracy after \", cv, \" folds with \", num_bins, \" bins per EP: \", round(np.mean(total_acc), 2), \" % ||\")\n",
    "print(\"|| FAMILY: \", family_to_select,  \" || Bins per EP: \", num_bins, \" ||\")\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f2698-8a26-4212-90b2-d42cbead5678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
