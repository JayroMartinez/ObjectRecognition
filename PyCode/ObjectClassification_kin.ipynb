{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a94b42-b23b-4c70-98ad-ae87e1a98245",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import glmnet_python\n",
    "# from glmnet_python import glmnet\n",
    "# from glmnet_python import glmnetPredict\n",
    "# from glmnet_python import glmnetPlot\n",
    "# from glmnet_python import cvglmnet\n",
    "# from glmnet_python import cvglmnetPredict\n",
    "# from glmnet_python import cvglmnetPlot\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import codecs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af78493b-84af-46cd-9742-f9ca13de89ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VARIABLES TO ADJUST\n",
    "# 'Mugs', 'Plates', 'Geometric', 'Cutlery', 'Ball'\n",
    "family_to_select = 'Ball'\n",
    "target = 'Middle Abd'\n",
    "num_bins = 20\n",
    "cv = 5\n",
    "# C_par [0.2, 0.5, 1, 1.25, 1.5]\n",
    "C_par = 0.2\n",
    "# NMF Syns [4, 8, 12, 16, 24, 32, 48, 64]\n",
    "nmf_syns = 4\n",
    "l1vsl2 = 0.5\n",
    "kin_labels = [\"Thumb Rotate\", \"Thumb MPJ\", \"Thumb IJ\", \"Index MPJ\", \"Index PIJ\", \"Middle MPJ\", \"Middle PIJ\", \"Ring MPJ\", \"Ring PIJ\", \"Pinkie MPJ\", \"Pinkie PIJ\", \"Palm Arch\", \"Wrist Pitch\", \"Wrist Yaw\", \"Index Abd\", \"Pinkie Abd\", \"Ring Abd\", \"Middle Abd\", \"Thumb Abd\"]\n",
    "emg_labels = ['emg' + str(i) for i in range(0,64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2a31b9-94d3-42e3-bd63-be6469db73d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# READING FILES AND CREATING DATASET\n",
    "file_data = \"./PyData/filtered_data.json\"\n",
    "file_eps = \"./PyData/ep_labels.json\"\n",
    "file_task = \"./PyData/task_labels.json\"\n",
    "file_allSyn = \"./PyData/all_subjects_scores.json\"\n",
    "file_emg = \"./PyData/emg_data.json\"\n",
    "file_nmf = \"./PyData/W_\"+str(nmf_syns)+\".json\"\n",
    "\n",
    "with open(file_data, \"r\") as f:\n",
    "    data = json.load(f)  # data[subjects][trials][joints]\n",
    "\n",
    "with open(file_eps, \"r\") as g:\n",
    "    eps = json.load(g)  # eps[subjects][trials]\n",
    "\n",
    "with open(file_task, \"r\") as h:\n",
    "    task = json.load(h)  # task[subjects][trials]\n",
    "\n",
    "with open(file_allSyn, \"r\") as h:\n",
    "    all_syn = json.load(h)  # all_syn[subjects X trials][joints]\n",
    "    \n",
    "with open(file_emg, \"r\") as f:\n",
    "    emg_data = json.load(f)  # data[subjects][trials][sensors]\n",
    "    \n",
    "with open(file_nmf, \"r\") as f:\n",
    "    nmf_data = json.load(f)  # nmf_data[subjects X trials][nmf syns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17952a0b-0777-4983-917c-75f70cb0c92e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VECTORIZING DATA\n",
    "\n",
    "vectorized_task = [x for sublist in task for x in sublist]  # Vectorization of tasks\n",
    "vectorized_eps = [x for sublist in eps for x in sublist]  # Vectorization of eps\n",
    "vectorized_data = [x for sublist in data for x in sublist]  # Vectorization of trials\n",
    "vectorized_data = np.array(vectorized_data, dtype=float) # Conversion to float to we replace 'None' with 'NaN'\n",
    "vectorized_emg_data = [x for sublist in emg_data for x in sublist]  # Vectorization of trials\n",
    "vectorized_emg_data = np.array(vectorized_emg_data, dtype=float) # Conversion to float to we replace 'None' with 'NaN'\n",
    "given_object = [x.split(\"_\")[0] for x in vectorized_task]  # Vectorized given objects\n",
    "ask_object = [x.split(\"_\")[1] for x in vectorized_task]  # Vectorized asked objects\n",
    "all_syn = np.array(all_syn, dtype=float) # Conversion to float to replace 'None' with 'NaN\n",
    "# NMF require no preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d818fa-4ff8-4f40-9256-9d32ce86e41b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SPLIT BY TRIALS\n",
    "\n",
    "tr_idx = [index for index, _ in enumerate(vectorized_task) if vectorized_task[index] != vectorized_task[index-1]]\n",
    "tr_idx.append(len(vectorized_task))\n",
    "\n",
    "# all these lists are [trials]x[timepoints per trial]\n",
    "spl_task = [vectorized_task[tr_idx[i]:tr_idx[i+1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx)-1]\n",
    "spl_eps = [vectorized_eps[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_dat = [vectorized_data[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_emg = [vectorized_emg_data[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_given = [given_object[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_ask = [ask_object[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_syn = [all_syn[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "spl_nmf = [nmf_data[tr_idx[i]:tr_idx[i + 1]] for i, _ in enumerate(tr_idx) if i != len(tr_idx) - 1]\n",
    "\n",
    "# print(len(spl_task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8930f5cc-ac73-4285-881f-acd31406dfd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SELECT TRIALS BY FAMILY\n",
    "\n",
    "obj_fam = dict(\n",
    "    CeramicMug = 'Mugs',\n",
    "    Glass = 'Mugs',\n",
    "    MetalMug = 'Mugs',\n",
    "    CeramicPlate = 'Plates',\n",
    "    MetalPlate = 'Plates',\n",
    "    PlasticPlate = 'Plates',\n",
    "    Cube = 'Geometric',\n",
    "    Cylinder ='Geometric',\n",
    "    Triangle ='Geometric',\n",
    "    Fork = 'Cutlery',\n",
    "    Knife ='Cutlery',\n",
    "    Spoon ='Cutlery',\n",
    "    PingPongBall = 'Ball',\n",
    "    SquashBall='Ball',\n",
    "    TennisBall='Ball'\n",
    ")\n",
    "\n",
    "fam_idx = list()\n",
    "for it in range(len(spl_given)):\n",
    "    if obj_fam[spl_given[it][0]] == family_to_select:\n",
    "        fam_idx.append(it)\n",
    "\n",
    "selected_task = [spl_task[idx] for idx in fam_idx]\n",
    "selected_eps = [spl_eps[idx] for idx in fam_idx]\n",
    "selected_dat = [spl_dat[idx] for idx in fam_idx]\n",
    "selected_emg = [spl_emg[idx] for idx in fam_idx]\n",
    "selected_given = [spl_given[idx] for idx in fam_idx]\n",
    "selected_ask = [spl_ask[idx] for idx in fam_idx]\n",
    "selected_syn = [spl_syn[idx] for idx in fam_idx]\n",
    "selected_nmf = [spl_nmf[idx] for idx in fam_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b90a88-b48b-4191-9aba-acbd363e61f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flat_kin = [item for sublist in selected_z_dat for item in sublist]\n",
    "# flat_giv = [item for sublist in selected_given for item in sublist]\n",
    "\n",
    "# kin = stats.zscore(flat_kin, axis=0, nan_policy='omit')\n",
    "# aux_df = pd.DataFrame(kin, columns=kin_labels)\n",
    "# aux_df['Object'] = flat_giv\n",
    "# aux_df.head\n",
    "\n",
    "\n",
    "# sns.violinplot(data=aux_df, x=\"Object\", y=target, inner=\"quartile\")\n",
    "# # sns.set(rc={'figure.figsize':(48,27)})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80cab919-dfee-429c-84a9-f58e8a07f3f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4510/1570130290.py:47: RuntimeWarning: Mean of empty slice\n",
      "  dat_mean = [np.nanmean(x, axis=0) for x in div_dat]\n",
      "/tmp/ipykernel_4510/1570130290.py:48: RuntimeWarning: Mean of empty slice\n",
      "  syn_mean = [np.nanmean(x, axis=0) for x in div_syn]\n",
      "/tmp/ipykernel_4510/1570130290.py:49: RuntimeWarning: Mean of empty slice\n",
      "  emg_mean = [np.nanmean(x, axis=0) for x in div_emg]\n",
      "/tmp/ipykernel_4510/1570130290.py:50: RuntimeWarning: Mean of empty slice\n",
      "  nmf_mean = [np.nanmean(x, axis=0) for x in div_nmf]\n"
     ]
    }
   ],
   "source": [
    "# DIVIDE BY EPs\n",
    "\n",
    "# all these list are [trials]x[eps per trial]x[timepoints per ep]\n",
    "final_task = list()\n",
    "final_eps = list()\n",
    "final_given = list()\n",
    "final_ask = list()\n",
    "# this lists are [trials]x[eps per trial]x[bins per ep]x[joints]\n",
    "final_dat = list()\n",
    "final_emg = list()\n",
    "final_syn = list()\n",
    "final_nmf = list()\n",
    "\n",
    "for e in range(len(selected_eps)):\n",
    "\n",
    "    ch_idx = [index for index, _ in enumerate(selected_eps[e]) if selected_eps[e][index] != selected_eps[e][index-1]]\n",
    "    ch_idx.append(len(selected_eps[e]))\n",
    "    if 0 not in ch_idx:\n",
    "        ch_idx.insert(0, 0)\n",
    "\n",
    "    sel_task = [selected_task[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_task.append(sel_task)\n",
    "    sel_eps = [selected_eps[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_eps.append(sel_eps)\n",
    "    sel_given = [selected_given[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_given.append(sel_given)\n",
    "    sel_ask = [selected_ask[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    final_ask.append(sel_ask)\n",
    "\n",
    "    sel_dat = [selected_dat[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    sel_emg = [selected_emg[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    sel_syn = [selected_syn[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    sel_nmf = [selected_nmf[e][ch_idx[i]:ch_idx[i + 1]] for i, _ in enumerate(ch_idx) if i != len(ch_idx) - 1]\n",
    "    \n",
    "    # DIVIDE BY BINS\n",
    "    aux = list()\n",
    "    aux_syn = list()\n",
    "    aux_emg = list()\n",
    "    aux_nmf = list()\n",
    "    \n",
    "    for j in range(len(sel_dat)):\n",
    "        div_dat = np.array_split(sel_dat[j], num_bins)\n",
    "        div_emg = np.array_split(sel_emg[j], num_bins)\n",
    "        div_syn = np.array_split(sel_syn[j], num_bins)\n",
    "        div_nmf = np.array_split(sel_nmf[j], num_bins)\n",
    "        \n",
    "        dat_mean = [np.nanmean(x, axis=0) for x in div_dat]\n",
    "        syn_mean = [np.nanmean(x, axis=0) for x in div_syn]\n",
    "        emg_mean = [np.nanmean(x, axis=0) for x in div_emg]\n",
    "        nmf_mean = [np.nanmean(x, axis=0) for x in div_nmf]\n",
    "        \n",
    "        aux.append(dat_mean)\n",
    "        aux_syn.append(syn_mean)\n",
    "        aux_emg.append(emg_mean)\n",
    "        aux_nmf.append(nmf_mean)\n",
    "        \n",
    "        \n",
    "    final_dat.append(aux)\n",
    "    final_syn.append(aux_syn)\n",
    "    final_emg.append(aux_emg)\n",
    "    final_nmf.append(aux_nmf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec5e22a-2475-4431-b1da-323e3837a196",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  32  out of  59 .  54.24 %\n",
      "Hits:  21  out of  36 .  58.33 %\n",
      "Hits:  19  out of  42 .  45.24 %\n",
      "Hits:  13  out of  30 .  43.33 %\n",
      "Hits:  14  out of  26 .  53.85 %\n",
      "|| FAMILY:  Ball  || Mean accuracy after  5  folds with  20  bins per EP:  51.0  % ||\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# SELECT TRIALS CV\n",
    "\n",
    "total_acc = list()\n",
    "unique_given = [final_given[i][0][0] for i in range(len(final_given))]  # unique object per trial\n",
    "skf = StratifiedKFold(n_splits=cv)\n",
    "for train, test in skf.split(final_dat, unique_given):\n",
    "\n",
    "    train_given = [unique_given[x] for x in train]\n",
    "    test_given = [unique_given[y] for y in test]\n",
    "    train_data = [final_dat[x] for x in train]\n",
    "    test_data = [final_dat[y] for y in test]\n",
    "    train_emg = [final_emg[x] for x in train]\n",
    "    test_emg = [final_emg[y] for y in test]\n",
    "    train_syn = [final_syn[x] for x in train]\n",
    "    test_syn = [final_syn[y] for y in test]\n",
    "    train_nmf = [final_nmf[x] for x in train]\n",
    "    test_nmf = [final_nmf[y] for y in test]\n",
    "\n",
    "    # FOR SINGLE SOURCE CLASSIFICATION\n",
    "    trn_dat = list()\n",
    "    trn_emg = list()\n",
    "    trn_lab = list()\n",
    "    trn_syn = list()\n",
    "    trn_nmf = list()\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        for j in range(len(train_data[i])):\n",
    "            aux = list()\n",
    "            aux_syn = list()\n",
    "            aux_emg = list()\n",
    "            aux_nmf = list()\n",
    "            for k in range(len(train_data[i][j])):\n",
    "                aux = np.append(aux, train_data[i][j][k])\n",
    "                aux_syn = np.append(aux_syn, train_syn[i][j][k])\n",
    "                aux_emg = np.append(aux_emg, train_emg[i][j][k])\n",
    "                aux_nmf = np.append(aux_nmf, train_nmf[i][j][k])\n",
    "            if np.count_nonzero(np.isnan(aux)) == 0 and np.count_nonzero(np.isnan(aux_syn)) == 0 and np.count_nonzero(np.isnan(aux_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "                trn_dat.append(aux)  # size = Joints X bins\n",
    "                trn_lab.append(train_given[i])\n",
    "                trn_syn.append(aux_syn)  # size = Synergies X bins\n",
    "                trn_emg.append(aux_emg)  # size = Electrodes X bins\n",
    "                trn_nmf.append(aux_nmf)  # size = NMF Syns X bins\n",
    "\n",
    "    tst_dat = list()\n",
    "    tst_emg = list()\n",
    "    tst_lab = list()\n",
    "    tst_syn = list()\n",
    "    tst_nmf = list()\n",
    "    for i2 in range(len(test_data)):\n",
    "        for j2 in range(len(test_data[i2])):\n",
    "            aux2 = list()\n",
    "            aux2_syn = list()\n",
    "            aux2_emg = list()\n",
    "            aux2_nmf = list()\n",
    "            for k2 in range(len(test_data[i2][j2])):\n",
    "                aux2 = np.append(aux2, test_data[i2][j2][k2])\n",
    "                aux2_syn = np.append(aux2_syn, test_syn[i2][j2][k2])\n",
    "                aux2_emg = np.append(aux2_emg, test_emg[i2][j2][k2])\n",
    "                aux2_nmf = np.append(aux2_nmf, test_nmf[i2][j2][k2])\n",
    "            if np.count_nonzero(np.isnan(aux2)) == 0 and np.count_nonzero(np.isnan(aux2_syn)) == 0 and np.count_nonzero(np.isnan(aux2_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "                tst_dat.append(aux2)\n",
    "                tst_lab.append(test_given[i2])\n",
    "                tst_syn.append(aux2_syn)\n",
    "                tst_emg.append(aux2_emg)\n",
    "                tst_nmf.append(aux2_nmf)\n",
    "    \n",
    "    # USING LOGISTIC REGRESSION\n",
    "    # log_model = LogisticRegression(penalty='elasticnet', C=C_par, solver='saga', max_iter=25000, multi_class='multinomial', l1_ratio=l1vsl2)\n",
    "    log_model = LogisticRegression(penalty='elasticnet', C=C_par, solver='saga', max_iter=25000, multi_class='multinomial', l1_ratio=l1vsl2)\n",
    "    log_model.fit(trn_dat, trn_lab)\n",
    "    pred = log_model.predict_proba(tst_dat)\n",
    "\n",
    "    cl = log_model.classes_\n",
    "    hits = 0\n",
    "    for i in range(len(pred)):\n",
    "        if cl[np.argmax(pred[i])] == tst_lab[i]:\n",
    "            hits += 1\n",
    "    acc = round((hits/len(tst_lab))*100, 2)\n",
    "    total_acc.append(acc)\n",
    "    print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "    \n",
    "    # WRITE WEIGHTS INTO A FILE\n",
    "    # file_name = \"./PyData/\" + family_to_select + \"_NMF_\" + nmf_syns + \".csv\"\n",
    "    # file_name = \"./PyData/NMF_weights_\" + family_to_select + \".csv\"\n",
    "    # weights = log_model.coef_\n",
    "    # resh = weights.reshape((weights.shape[0], int(weights.shape[1]/num_bins), num_bins))\n",
    "    # mean_resh = np.mean(resh, axis=2)\n",
    "    # df = pd.DataFrame(mean_resh, index=log_model.classes_)\n",
    "    # df.to_csv(file_name, index=True, header=None, mode='a')\n",
    "    # print(\"FILE UPDATED\")\n",
    "\n",
    "#     #  USING GLMNET\n",
    "#     # lbl = LabelEncoder()\n",
    "#     # dum = np.float64(lbl.fit_transform(trn_lab))\n",
    "#     #\n",
    "#     # cvfit = cvglmnet(x=np.array(trn_dat), y=dum, family='multinomial', alpha=0.5)\n",
    "#     # pred = cvglmnetPredict(cvfit, np.array(tst_dat), ptype='class')\n",
    "#     #\n",
    "#     # dum_res = np.float64(lbl.fit_transform(tst_lab))\n",
    "#     # hits = 0\n",
    "#     # for i in range(len(pred)):\n",
    "#     #     if pred[i] == dum_res[i]:\n",
    "#     #         hits += 1\n",
    "#     # acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     # total_acc.append(acc)\n",
    "#     # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "#     # USING SVM\n",
    "#     # clf = svm.SVC(C=25, kernel='poly', degree=3, decision_function_shape='ovo')  # 45.91\n",
    "#     # clf.fit(np.array(trn_dat), trn_lab)\n",
    "#     # pred = clf.predict(np.array(tst_dat))\n",
    "#     # hits = 0\n",
    "#     # for i in range(len(pred)):\n",
    "#     #     if pred[i] == tst_lab[i]:\n",
    "#     #         hits += 1\n",
    "#     # acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     # total_acc.append(acc)\n",
    "#     # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "#     # USING ANN\n",
    "#     # num_hidden_layers = 4\n",
    "#     # layers = np.ceil(np.geomspace(len(trn_dat[0]), 3, num_hidden_layers+2)).astype(int)\n",
    "#     # clf = MLPClassifier(solver='adam', alpha=20, hidden_layer_sizes=(layers[1:len(layers)-1]), random_state=1, max_iter=100000, activation='tanh')\n",
    "#     # clf.fit(np.array(trn_dat), trn_lab)\n",
    "#     # pred = clf.predict(np.array(tst_dat))\n",
    "#     # hits = 0\n",
    "#     # for i in range(len(pred)):\n",
    "#     #     if pred[i] == tst_lab[i]:\n",
    "#     #         hits += 1\n",
    "#     # acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     # total_acc.append(acc)\n",
    "#     # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "# ##########################################################################\n",
    "#     # FOR MULTIPLE SOURCE CLASSIFICATION\n",
    "#     trn_kin = list()\n",
    "#     trn_emg = list()\n",
    "    \n",
    "#     trn_lab = list()\n",
    "\n",
    "#     for i in range(len(train_data)):\n",
    "#         for j in range(len(train_data[i])):\n",
    "#             aux = list()\n",
    "#             aux_emg = list()\n",
    "\n",
    "#             for k in range(len(train_data[i][j])):\n",
    "#                 aux = np.append(aux, train_data[i][j][k])\n",
    "#                 aux_emg = np.append(aux_emg, train_emg[i][j][k])\n",
    "                \n",
    "#             if np.count_nonzero(np.isnan(aux)) == 0 and np.count_nonzero(np.isnan(aux_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "#                 trn_lab.append(train_given[i])\n",
    "#                 trn_kin.append(aux) \n",
    "#                 trn_emg.append(aux_emg)\n",
    "                \n",
    "                \n",
    "#     trn_dat = np.concatenate((trn_kin, trn_emg), axis=1)\n",
    "\n",
    "\n",
    "#     # tst_dat = list()\n",
    "#     tst_kin = list()\n",
    "#     tst_emg = list()\n",
    "#     tst_lab = list()\n",
    "#     for i2 in range(len(test_data)):\n",
    "#         for j2 in range(len(test_data[i2])):\n",
    "#             aux2 = list()\n",
    "#             aux2_emg = list()\n",
    "#             for k2 in range(len(test_data[i2][j2])):\n",
    "#                 aux2 = np.append(aux2, test_data[i2][j2][k2])\n",
    "#                 aux2_emg = np.append(aux2_emg, test_emg[i2][j2][k2])\n",
    "                \n",
    "#             if np.count_nonzero(np.isnan(aux2)) == 0 and np.count_nonzero(np.isnan(aux2_emg)) == 0: # DISCARD EPs WITH ALL KINEMATICS OR ALL SYNERGIES AS NaN\n",
    "#                 tst_kin.append(aux2)\n",
    "#                 tst_emg.append(aux2_emg)\n",
    "#                 tst_lab.append(test_given[i2])\n",
    "                \n",
    "                \n",
    "#     tst_dat = np.concatenate((tst_kin, tst_emg), axis=1)\n",
    "                \n",
    "#     # USING LOGISTIC REGRESSION\n",
    "#     log_model = LogisticRegression(penalty='elasticnet', C=C_par, solver='saga', max_iter=25000, multi_class='multinomial', n_jobs=-1, l1_ratio=l1vsl2)\n",
    "#     log_model.fit(trn_dat_norm, trn_lab)\n",
    "#     weights = log_model.coef_\n",
    "#     # print(\"Size Weights: \", weights.shape)\n",
    "#     # print(\"Classes: \", log_model.classes_)\n",
    "    \n",
    "#     # WRITE WEIGHTS INTO A FILE\n",
    "#     file_name = \"./PyData/KIN_EMG_Norm_weights_N05_\" + family_to_select + \".csv\"\n",
    "#     # weights = log_model.coef_\n",
    "#     resh = weights.reshape((weights.shape[0], int(weights.shape[1]/num_bins), num_bins))\n",
    "#     mean_resh = np.mean(np.abs(resh), axis=2)\n",
    "#     df = pd.DataFrame(mean_resh, index=log_model.classes_)\n",
    "#     df.to_csv(file_name, index=True, header=None, mode='a')\n",
    "#     print(\"FILE UPDATED\")\n",
    "    \n",
    "#     # GET PREDICTIONS\n",
    "#     pred = log_model.predict_proba(tst_dat_norm)\n",
    "#     cl = log_model.classes_\n",
    "#     hits = 0\n",
    "#     for i in range(len(pred)):\n",
    "#         if cl[np.argmax(pred[i])] == tst_lab[i]:\n",
    "#             hits += 1\n",
    "#     acc = round((hits/len(tst_lab))*100, 2)\n",
    "#     total_acc.append(acc)\n",
    "#     print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "    #  USING GLMNET\n",
    "    # lbl = LabelEncoder()\n",
    "    # dum = np.float64(lbl.fit_transform(trn_lab))\n",
    "    #\n",
    "    # cvfit = cvglmnet(x=np.array(trn_dat), y=dum, family='multinomial', alpha=0.5)\n",
    "    # pred = cvglmnetPredict(cvfit, np.array(tst_dat), ptype='class')\n",
    "    #\n",
    "    # dum_res = np.float64(lbl.fit_transform(tst_lab))\n",
    "    # hits = 0\n",
    "    # for i in range(len(pred)):\n",
    "    #     if pred[i] == dum_res[i]:\n",
    "    #         hits += 1\n",
    "    # acc = round((hits/len(tst_lab))*100, 2)\n",
    "    # total_acc.append(acc)\n",
    "    # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "    # USING SVM\n",
    "    # clf = svm.SVC(C=25, kernel='poly', degree=3, decision_function_shape='ovo')  # 45.91\n",
    "    # clf.fit(np.array(trn_dat), trn_lab)\n",
    "    # pred = clf.predict(np.array(tst_dat))\n",
    "    # hits = 0\n",
    "    # for i in range(len(pred)):\n",
    "    #     if pred[i] == tst_lab[i]:\n",
    "    #         hits += 1\n",
    "    # acc = round((hits/len(tst_lab))*100, 2)\n",
    "    # total_acc.append(acc)\n",
    "    # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "    # USING ANN\n",
    "    # num_hidden_layers = 4\n",
    "    # layers = np.ceil(np.geomspace(len(trn_dat[0]), 3, num_hidden_layers+2)).astype(int)\n",
    "    # clf = MLPClassifier(solver='adam', alpha=20, hidden_layer_sizes=(layers[1:len(layers)-1]), random_state=1, max_iter=100000, activation='tanh')\n",
    "    # clf.fit(np.array(trn_dat), trn_lab)\n",
    "    # pred = clf.predict(np.array(tst_dat))\n",
    "    # hits = 0\n",
    "    # for i in range(len(pred)):\n",
    "    #     if pred[i] == tst_lab[i]:\n",
    "    #         hits += 1\n",
    "    # acc = round((hits/len(tst_lab))*100, 2)\n",
    "    # total_acc.append(acc)\n",
    "    # print(\"Hits: \", hits, \" out of \", len(tst_lab), \". \", acc, \"%\")\n",
    "\n",
    "print(\"|| FAMILY: \", family_to_select, \" || Mean accuracy after \", cv, \" folds with \", num_bins, \" bins per EP: \", round(np.mean(total_acc), 2), \" % ||\")\n",
    "# print(\"|| FAMILY: \", family_to_select,  \" || Bins per EP: \", num_bins, \" ||\")\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3468da8d-f953-4d00-b391-fb376f21a489",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### KINEMATICS\n",
    "# kin_labels = [\"Thumb Rotate\", \"Thumb MPJ\", \"Thumb IJ\", \"Index MPJ\", \"Index PIJ\", \"Middle MPJ\", \"Middle PIJ\", \"Ring MPJ\", \"Ring PIJ\", \"Pinkie MPJ\", \"Pinkie PIJ\", \"Palm Arch\", \"Wrist Pitch\", \"Wrist Yaw\", \"Index Abd\", \"Pinkie Abd\", \"Ring Abd\", \"Middle Abd\", \"Thumb Abd\"]\n",
    "# z_score_kin = stats.zscore(vectorized_data, axis=0, nan_policy='omit')\n",
    "# new_df = pd.DataFrame(z_score_kin, columns=kin_labels)\n",
    "# new_df['Object'] = given_object\n",
    "\n",
    "# # select trials by object\n",
    "# selected = new_df.loc[(new_df['Object'] == 'CeramicMug') | (new_df['Object'] == 'Glass') | (new_df['Object'] == 'MetalMug')]\n",
    "\n",
    "# res_anova_index = pg.anova(data=selected, dv='Index MPJ', between='Object', detailed=True)\n",
    "# res_pairwise_index = pg.pairwise_tukey(data=selected, dv='Index MPJ', between='Object')\n",
    "# print(res_anova_index['p-unc'].iloc[0])\n",
    "# print(res_pairwise_index['p-tukey'])\n",
    "\n",
    "# res_anova_thumb = pg.anova(data=selected, dv='Thumb Abd', between='Object', detailed=True)\n",
    "# res_pairwise_thumb = pg.pairwise_tukey(data=selected, dv='Thumb Abd', between='Object')\n",
    "# # print(res_anova_thumb)\n",
    "# # print(res_pairwise_thumb)\n",
    "\n",
    "# sns.violinplot(data=selected, x=\"Object\", y=\"Index MPJ\", inner=\"quartile\")\n",
    "# sns.set(rc={'figure.figsize':(48,27)})\n",
    "# plt.show()\n",
    "\n",
    "### EMG\n",
    "# emg_labels = ['emg' + str(i) for i in range(0,64)]\n",
    "# z_score_emg = stats.zscore(vectorized_emg_data, axis=0, nan_policy='omit')\n",
    "# new_df_emg = pd.DataFrame(z_score_emg, columns=emg_labels)\n",
    "# new_df_emg['Object'] = given_object\n",
    "# # print(new_df_emg)\n",
    "\n",
    "# selected_emg = new_df_emg.loc[(new_df_emg['Object'] == 'CeramicMug') | (new_df_emg['Object'] == 'Glass') | (new_df_emg['Object'] == 'MetalMug')]\n",
    "# # print(selected_emg['emg18'])\n",
    "# res_anova_index_emg = pg.anova(data=selected_emg, dv='emg18', between='Object', detailed=True)\n",
    "# res_pairwise_index_emg = pg.pairwise_tukey(data=selected_emg, dv='emg18', between='Object')\n",
    "# print(res_anova_index_emg)\n",
    "# print(res_pairwise_index_emg)\n",
    "# # print(res_anova_index_emg['p-unc'].iloc[0])\n",
    "# # print(res_pairwise_index_emg['p-tukey'])\n",
    "\n",
    "\n",
    "# sns.violinplot(data=selected_emg, x=\"Object\", y=\"emg18\", inner=\"quartile\")\n",
    "# sns.set(rc={'figure.figsize':(48,27)})\n",
    "# plt.show()\n",
    "\n",
    "# tot_dat = np.concatenate((trn_dat_norm, tst_dat_norm), axis=0)\n",
    "# obj_labels = np.append(trn_lab, tst_lab)\n",
    "\n",
    "# sensor_labels = np.append(kin_labels, emg_labels)\n",
    "\n",
    "# fin_tot_dat = list()\n",
    "# labs = list()\n",
    "# for i in range(len(tot_dat)):\n",
    "#     aux = np.array_split(tot_dat[i], num_bins)\n",
    "#     [fin_tot_dat.append(iter) for iter in aux]\n",
    "#     [labs.append(obj_labels[i]) for iter in aux]\n",
    "#     # labs.append(np.repeat(labels[i], num_bins))\n",
    "\n",
    "# new_df = pd.DataFrame(fin_tot_dat, columns=sensor_labels)\n",
    "# new_df['Object'] = labs\n",
    "\n",
    "# # target = 'Middle Abd'\n",
    "\n",
    "# res_anova_1 = pg.anova(data=new_df, dv=target, between='Object', detailed=True).round(3)\n",
    "# res_pairwise_t = pg.pairwise_tukey(data=new_df, dv=target, between='Object').round(3)\n",
    "# res_pairwise_1 = pg.pairwise_tests(data=new_df, dv=target, between='Object', padjust='bonf').round(3)\n",
    "\n",
    "# print(\"Target: \", target) \n",
    "\n",
    "# print(\"\\nANOVA p-val: \",  str(res_anova_1['p-unc'][0]))\n",
    "# print(\"\\nTukey Pairwise p-val:\")\n",
    "# for it in range(res_pairwise_t.shape[0]):\n",
    "#     print(str(res_pairwise_t['A'][it]), str(res_pairwise_t['B'][it]), str(res_pairwise_t['p-tukey'][it]))\n",
    "    \n",
    "# print(\"\\nBonferroni t-test p-val:\")\n",
    "# for it in range(res_pairwise_1.shape[0]):\n",
    "#     print(str(res_pairwise_1['A'][it]) + \" \" + str(res_pairwise_1['B'][it]) + \" \" + str(res_pairwise_1['p-corr'][it]))\n",
    "    \n",
    "# sns.violinplot(data=new_df, x=\"Object\", y=target, inner=\"quartile\")\n",
    "# # sns.set(rc={'figure.figsize':(48,27)})\n",
    "# plt.show()\n",
    "\n",
    "# save_file = \"./PyData/Boxplot_\" + family_to_select + \"_\" + target.replace(\" \", \"\") + \".png\"\n",
    "# # plt.savefig(save_file, bbox_inches=\"tight\")\n",
    "# print(\"\\nSaved \", save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
